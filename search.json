[
  {
    "objectID": "Lab02.html",
    "href": "Lab02.html",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "(Adapted from ISLR Chapter 3 Lab: Introduction to R)\n\n\n\nA=matrix(1:16,4,4)\nA\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n[3,]    3    7   11   15\n[4,]    4    8   12   16\n\nA[2,3]\n\n[1] 10\n\nA[c(1,3),c(2,4)]\n\n     [,1] [,2]\n[1,]    5   13\n[2,]    7   15\n\nA[1:3,2:4]\n\n     [,1] [,2] [,3]\n[1,]    5    9   13\n[2,]    6   10   14\n[3,]    7   11   15\n\nA[1:2,]\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n\nA[,1:2]\n\n     [,1] [,2]\n[1,]    1    5\n[2,]    2    6\n[3,]    3    7\n[4,]    4    8\n\nA[1,]\n\n[1]  1  5  9 13\n\nA[-c(1,3),] # What does -c() do?\n\n     [,1] [,2] [,3] [,4]\n[1,]    2    6   10   14\n[2,]    4    8   12   16\n\n# exclude rows 1 and 3 from the matrix A.\nA[-c(1,3),-c(1,3,4)]\n\n[1] 6 8\n\ndim(A) # Dimensions\n\n[1] 4 4\n\n\n\n\n\n\nAuto=read.table(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.data\")\nAuto=read.table(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.data\",header=T,na.strings=\"?\") \nAuto=read.csv(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.csv\") # read csv file\n# Which function reads data faster?\n# read.csv generally reads faster than read.table for CSVs\n\n# Try using this simple method\n# time1 = proc.time()\n# Auto=read.csv(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.csv\",header=T,na.strings=\"?\")\n# proc.time()-time1\n\n# Check on data\ndim(Auto)\n\n[1] 397   9\n\nAuto[1:4,] # select rows\n\n  mpg cylinders displacement horsepower weight acceleration year origin\n1  18         8          307        130   3504         12.0   70      1\n2  15         8          350        165   3693         11.5   70      1\n3  18         8          318        150   3436         11.0   70      1\n4  16         8          304        150   3433         12.0   70      1\n                       name\n1 chevrolet chevelle malibu\n2         buick skylark 320\n3        plymouth satellite\n4             amc rebel sst\n\nAuto=na.omit(Auto)\ndim(Auto) # Notice the difference?\n\n[1] 397   9\n\n# Fewer rows after NA removal\nnames(Auto)\n\n[1] \"mpg\"          \"cylinders\"    \"displacement\" \"horsepower\"   \"weight\"      \n[6] \"acceleration\" \"year\"         \"origin\"       \"name\"        \n\n\n\n\n\n\nAuto=read.table(\"https://www.statlearning.com/s/Auto.data\",header=T,na.strings=\"?\")\ndim(Auto)\n\n[1] 397   9\n\n\n\n\n\n\n# plot(cylinders, mpg)\nplot(Auto$cylinders, Auto$mpg)\n\n\n\n\n\n\n\nattach(Auto)\nplot(cylinders, mpg)\n\n\n\n\n\n\n\ncylinders=as.factor(cylinders)\nplot(cylinders, mpg)\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\")\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T)\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T,horizontal=T)\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T, xlab=\"cylinders\", ylab=\"MPG\")\n\n\n\n\n\n\n\nhist(mpg)\n\n\n\n\n\n\n\nhist(mpg,col=2)\n\n\n\n\n\n\n\nhist(mpg,col=2,breaks=15)\n\n\n\n\n\n\n\n#pairs(Auto)\npairs(~ mpg + displacement + horsepower + weight + acceleration, Auto)\n\n\n\n\n\n\n\nplot(horsepower,mpg)\n\n\n\n\n\n\n\n# identify(horsepower,mpg,name) # Interactive: point and click the dot to identify cases\nsummary(Auto)\n\n      mpg          cylinders      displacement     horsepower        weight    \n Min.   : 9.00   Min.   :3.000   Min.   : 68.0   Min.   : 46.0   Min.   :1613  \n 1st Qu.:17.50   1st Qu.:4.000   1st Qu.:104.0   1st Qu.: 75.0   1st Qu.:2223  \n Median :23.00   Median :4.000   Median :146.0   Median : 93.5   Median :2800  \n Mean   :23.52   Mean   :5.458   Mean   :193.5   Mean   :104.5   Mean   :2970  \n 3rd Qu.:29.00   3rd Qu.:8.000   3rd Qu.:262.0   3rd Qu.:126.0   3rd Qu.:3609  \n Max.   :46.60   Max.   :8.000   Max.   :455.0   Max.   :230.0   Max.   :5140  \n                                                 NA's   :5                     \n  acceleration        year           origin          name          \n Min.   : 8.00   Min.   :70.00   Min.   :1.000   Length:397        \n 1st Qu.:13.80   1st Qu.:73.00   1st Qu.:1.000   Class :character  \n Median :15.50   Median :76.00   Median :1.000   Mode  :character  \n Mean   :15.56   Mean   :75.99   Mean   :1.574                     \n 3rd Qu.:17.10   3rd Qu.:79.00   3rd Qu.:2.000                     \n Max.   :24.80   Max.   :82.00   Max.   :3.000                     \n                                                                   \n\nsummary(mpg)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   9.00   17.50   23.00   23.52   29.00   46.60 \n\n\n\n\n\n\nptbu=c(\"MASS\",\"ISLR\")\ninstall.packages(ptbu, repos='http://cran.us.r-project.org')\n\nInstalling packages into 'C:/Users/bihan/AppData/Local/R/win-library/4.5'\n(as 'lib' is unspecified)\n\n\npackage 'MASS' successfully unpacked and MD5 sums checked\n\n\nWarning: cannot remove prior installation of package 'MASS'\n\n\nWarning in file.copy(savedcopy, lib, recursive = TRUE): problem copying\nC:\\Users\\bihan\\AppData\\Local\\R\\win-library\\4.5\\00LOCK\\MASS\\libs\\x64\\MASS.dll to\nC:\\Users\\bihan\\AppData\\Local\\R\\win-library\\4.5\\MASS\\libs\\x64\\MASS.dll:\nPermission denied\n\n\nWarning: restored 'MASS'\n\n\npackage 'ISLR' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n    C:\\Users\\bihan\\AppData\\Local\\Temp\\RtmpYTg9rU\\downloaded_packages\n\nlapply(ptbu, require, character.only = TRUE)\n\nLoading required package: MASS\n\n\nLoading required package: ISLR\n\n\n\nAttaching package: 'ISLR'\n\n\nThe following object is masked _by_ '.GlobalEnv':\n\n    Auto\n\n\n[[1]]\n[1] TRUE\n\n[[2]]\n[1] TRUE\n\nlibrary(MASS)\nlibrary(ISLR)\n\n# Simple Linear Regression\n\n# fix(Boston)\nnames(Boston)\n\n [1] \"crim\"    \"zn\"      \"indus\"   \"chas\"    \"nox\"     \"rm\"      \"age\"    \n [8] \"dis\"     \"rad\"     \"tax\"     \"ptratio\" \"black\"   \"lstat\"   \"medv\"   \n\n# lm.fit=lm(medv~lstat)\nattach(Boston)\nlm.fit=lm(medv~lstat,data=Boston)\nattach(Boston)\n\nThe following objects are masked from Boston (pos = 3):\n\n    age, black, chas, crim, dis, indus, lstat, medv, nox, ptratio, rad,\n    rm, tax, zn\n\nlm.fit=lm(medv~lstat)\nlm.fit\n\n\nCall:\nlm(formula = medv ~ lstat)\n\nCoefficients:\n(Intercept)        lstat  \n      34.55        -0.95  \n\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ lstat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.168  -3.990  -1.318   2.034  24.500 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 34.55384    0.56263   61.41   &lt;2e-16 ***\nlstat       -0.95005    0.03873  -24.53   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.216 on 504 degrees of freedom\nMultiple R-squared:  0.5441,    Adjusted R-squared:  0.5432 \nF-statistic: 601.6 on 1 and 504 DF,  p-value: &lt; 2.2e-16\n\nnames(lm.fit)\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\ncoef(lm.fit)\n\n(Intercept)       lstat \n 34.5538409  -0.9500494 \n\nconfint(lm.fit)\n\n                2.5 %     97.5 %\n(Intercept) 33.448457 35.6592247\nlstat       -1.026148 -0.8739505\n\npredict(lm.fit,data.frame(lstat=(c(5,10,15))), interval=\"confidence\")\n\n       fit      lwr      upr\n1 29.80359 29.00741 30.59978\n2 25.05335 24.47413 25.63256\n3 20.30310 19.73159 20.87461\n\npredict(lm.fit,data.frame(lstat=(c(5,10,15))), interval=\"prediction\")\n\n       fit       lwr      upr\n1 29.80359 17.565675 42.04151\n2 25.05335 12.827626 37.27907\n3 20.30310  8.077742 32.52846\n\n# What is the differnce between \"conference\" and \"prediction\" difference?\n# Confidence Interval: Range of average mpg for all cars with 150 horsepower. Prediction Interval: Range where one new car’s mpg with 150 horsepower might fall.\n\nplot(lstat,medv)\nabline(lm.fit)\nabline(lm.fit,lwd=3)\nabline(lm.fit,lwd=3,col=\"red\")\n\n\n\n\n\n\n\nplot(lstat,medv,col=\"red\")\n\n\n\n\n\n\n\nplot(lstat,medv,pch=16)\n\n\n\n\n\n\n\nplot(lstat,medv,pch=\"+\")\n\n\n\n\n\n\n\nplot(1:20,1:20,pch=1:20)\n\n\n\n\n\n\n\npar(mfrow=c(2,2))\nplot(lm.fit)\n\n\n\n\n\n\n\nplot(predict(lm.fit), residuals(lm.fit))\nplot(predict(lm.fit), rstudent(lm.fit))\nplot(hatvalues(lm.fit))\nwhich.max(hatvalues(lm.fit))\n\n375 \n375 \n\n\n\n\n\n\n\n\n\n\n\n\n\nlm.fit=lm(medv~lstat+age,data=Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ lstat + age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.981  -3.978  -1.283   1.968  23.158 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 33.22276    0.73085  45.458  &lt; 2e-16 ***\nlstat       -1.03207    0.04819 -21.416  &lt; 2e-16 ***\nage          0.03454    0.01223   2.826  0.00491 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.173 on 503 degrees of freedom\nMultiple R-squared:  0.5513,    Adjusted R-squared:  0.5495 \nF-statistic:   309 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\nlm.fit=lm(medv~.,data=Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ ., data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.595  -2.730  -0.518   1.777  26.199 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.646e+01  5.103e+00   7.144 3.28e-12 ***\ncrim        -1.080e-01  3.286e-02  -3.287 0.001087 ** \nzn           4.642e-02  1.373e-02   3.382 0.000778 ***\nindus        2.056e-02  6.150e-02   0.334 0.738288    \nchas         2.687e+00  8.616e-01   3.118 0.001925 ** \nnox         -1.777e+01  3.820e+00  -4.651 4.25e-06 ***\nrm           3.810e+00  4.179e-01   9.116  &lt; 2e-16 ***\nage          6.922e-04  1.321e-02   0.052 0.958229    \ndis         -1.476e+00  1.995e-01  -7.398 6.01e-13 ***\nrad          3.060e-01  6.635e-02   4.613 5.07e-06 ***\ntax         -1.233e-02  3.760e-03  -3.280 0.001112 ** \nptratio     -9.527e-01  1.308e-01  -7.283 1.31e-12 ***\nblack        9.312e-03  2.686e-03   3.467 0.000573 ***\nlstat       -5.248e-01  5.072e-02 -10.347  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.745 on 492 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7338 \nF-statistic: 108.1 on 13 and 492 DF,  p-value: &lt; 2.2e-16\n\nlibrary(car)\n\nLoading required package: carData\n\nvif(lm.fit)\n\n    crim       zn    indus     chas      nox       rm      age      dis \n1.792192 2.298758 3.991596 1.073995 4.393720 1.933744 3.100826 3.955945 \n     rad      tax  ptratio    black    lstat \n7.484496 9.008554 1.799084 1.348521 2.941491 \n\nlm.fit1=lm(medv~.-age,data=Boston)\nsummary(lm.fit1)\n\n\nCall:\nlm(formula = medv ~ . - age, data = Boston)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.6054  -2.7313  -0.5188   1.7601  26.2243 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  36.436927   5.080119   7.172 2.72e-12 ***\ncrim         -0.108006   0.032832  -3.290 0.001075 ** \nzn            0.046334   0.013613   3.404 0.000719 ***\nindus         0.020562   0.061433   0.335 0.737989    \nchas          2.689026   0.859598   3.128 0.001863 ** \nnox         -17.713540   3.679308  -4.814 1.97e-06 ***\nrm            3.814394   0.408480   9.338  &lt; 2e-16 ***\ndis          -1.478612   0.190611  -7.757 5.03e-14 ***\nrad           0.305786   0.066089   4.627 4.75e-06 ***\ntax          -0.012329   0.003755  -3.283 0.001099 ** \nptratio      -0.952211   0.130294  -7.308 1.10e-12 ***\nblack         0.009321   0.002678   3.481 0.000544 ***\nlstat        -0.523852   0.047625 -10.999  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.74 on 493 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7343 \nF-statistic: 117.3 on 12 and 493 DF,  p-value: &lt; 2.2e-16\n\nlm.fit1=update(lm.fit, ~.-age)\n\n\n\n\n\nlm.fit2=lm(medv~lstat+I(lstat^2))\nsummary(lm.fit2)\n\n\nCall:\nlm(formula = medv ~ lstat + I(lstat^2))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.2834  -3.8313  -0.5295   2.3095  25.4148 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 42.862007   0.872084   49.15   &lt;2e-16 ***\nlstat       -2.332821   0.123803  -18.84   &lt;2e-16 ***\nI(lstat^2)   0.043547   0.003745   11.63   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.524 on 503 degrees of freedom\nMultiple R-squared:  0.6407,    Adjusted R-squared:  0.6393 \nF-statistic: 448.5 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\nlm.fit=lm(medv~lstat)\nanova(lm.fit,lm.fit2)\n\nAnalysis of Variance Table\n\nModel 1: medv ~ lstat\nModel 2: medv ~ lstat + I(lstat^2)\n  Res.Df   RSS Df Sum of Sq     F    Pr(&gt;F)    \n1    504 19472                                 \n2    503 15347  1    4125.1 135.2 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\npar(mfrow=c(2,2))\nplot(lm.fit2)\n\n\n\n\n\n\n\nlm.fit5=lm(medv~poly(lstat,5))\nsummary(lm.fit5)\n\n\nCall:\nlm(formula = medv ~ poly(lstat, 5))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.5433  -3.1039  -0.7052   2.0844  27.1153 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       22.5328     0.2318  97.197  &lt; 2e-16 ***\npoly(lstat, 5)1 -152.4595     5.2148 -29.236  &lt; 2e-16 ***\npoly(lstat, 5)2   64.2272     5.2148  12.316  &lt; 2e-16 ***\npoly(lstat, 5)3  -27.0511     5.2148  -5.187 3.10e-07 ***\npoly(lstat, 5)4   25.4517     5.2148   4.881 1.42e-06 ***\npoly(lstat, 5)5  -19.2524     5.2148  -3.692 0.000247 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.215 on 500 degrees of freedom\nMultiple R-squared:  0.6817,    Adjusted R-squared:  0.6785 \nF-statistic: 214.2 on 5 and 500 DF,  p-value: &lt; 2.2e-16\n\nsummary(lm(medv~log(rm),data=Boston))\n\n\nCall:\nlm(formula = medv ~ log(rm), data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-19.487  -2.875  -0.104   2.837  39.816 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -76.488      5.028  -15.21   &lt;2e-16 ***\nlog(rm)       54.055      2.739   19.73   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.915 on 504 degrees of freedom\nMultiple R-squared:  0.4358,    Adjusted R-squared:  0.4347 \nF-statistic: 389.3 on 1 and 504 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\n# fix(Carseats)\nnames(Carseats)\n\n [1] \"Sales\"       \"CompPrice\"   \"Income\"      \"Advertising\" \"Population\" \n [6] \"Price\"       \"ShelveLoc\"   \"Age\"         \"Education\"   \"Urban\"      \n[11] \"US\"         \n\nlm.fit=lm(Sales~.+Income:Advertising+Price:Age,data=Carseats)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = Sales ~ . + Income:Advertising + Price:Age, data = Carseats)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9208 -0.7503  0.0177  0.6754  3.3413 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         6.5755654  1.0087470   6.519 2.22e-10 ***\nCompPrice           0.0929371  0.0041183  22.567  &lt; 2e-16 ***\nIncome              0.0108940  0.0026044   4.183 3.57e-05 ***\nAdvertising         0.0702462  0.0226091   3.107 0.002030 ** \nPopulation          0.0001592  0.0003679   0.433 0.665330    \nPrice              -0.1008064  0.0074399 -13.549  &lt; 2e-16 ***\nShelveLocGood       4.8486762  0.1528378  31.724  &lt; 2e-16 ***\nShelveLocMedium     1.9532620  0.1257682  15.531  &lt; 2e-16 ***\nAge                -0.0579466  0.0159506  -3.633 0.000318 ***\nEducation          -0.0208525  0.0196131  -1.063 0.288361    \nUrbanYes            0.1401597  0.1124019   1.247 0.213171    \nUSYes              -0.1575571  0.1489234  -1.058 0.290729    \nIncome:Advertising  0.0007510  0.0002784   2.698 0.007290 ** \nPrice:Age           0.0001068  0.0001333   0.801 0.423812    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.011 on 386 degrees of freedom\nMultiple R-squared:  0.8761,    Adjusted R-squared:  0.8719 \nF-statistic:   210 on 13 and 386 DF,  p-value: &lt; 2.2e-16\n\nattach(Carseats)\ncontrasts(ShelveLoc)\n\n       Good Medium\nBad       0      0\nGood      1      0\nMedium    0      1\n\n\n\n\n\n\nsummary(lm(medv~lstat*age,data=Boston))\n\n\nCall:\nlm(formula = medv ~ lstat * age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.806  -4.045  -1.333   2.085  27.552 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 36.0885359  1.4698355  24.553  &lt; 2e-16 ***\nlstat       -1.3921168  0.1674555  -8.313 8.78e-16 ***\nage         -0.0007209  0.0198792  -0.036   0.9711    \nlstat:age    0.0041560  0.0018518   2.244   0.0252 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.149 on 502 degrees of freedom\nMultiple R-squared:  0.5557,    Adjusted R-squared:  0.5531 \nF-statistic: 209.3 on 3 and 502 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "Lab02.html#indexing-data-using",
    "href": "Lab02.html#indexing-data-using",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "A=matrix(1:16,4,4)\nA\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n[3,]    3    7   11   15\n[4,]    4    8   12   16\n\nA[2,3]\n\n[1] 10\n\nA[c(1,3),c(2,4)]\n\n     [,1] [,2]\n[1,]    5   13\n[2,]    7   15\n\nA[1:3,2:4]\n\n     [,1] [,2] [,3]\n[1,]    5    9   13\n[2,]    6   10   14\n[3,]    7   11   15\n\nA[1:2,]\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n\nA[,1:2]\n\n     [,1] [,2]\n[1,]    1    5\n[2,]    2    6\n[3,]    3    7\n[4,]    4    8\n\nA[1,]\n\n[1]  1  5  9 13\n\nA[-c(1,3),] # What does -c() do?\n\n     [,1] [,2] [,3] [,4]\n[1,]    2    6   10   14\n[2,]    4    8   12   16\n\n# exclude rows 1 and 3 from the matrix A.\nA[-c(1,3),-c(1,3,4)]\n\n[1] 6 8\n\ndim(A) # Dimensions\n\n[1] 4 4"
  },
  {
    "objectID": "Lab02.html#loading-data-from-github-remote",
    "href": "Lab02.html#loading-data-from-github-remote",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "Auto=read.table(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.data\")\nAuto=read.table(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.data\",header=T,na.strings=\"?\") \nAuto=read.csv(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.csv\") # read csv file\n# Which function reads data faster?\n# read.csv generally reads faster than read.table for CSVs\n\n# Try using this simple method\n# time1 = proc.time()\n# Auto=read.csv(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.csv\",header=T,na.strings=\"?\")\n# proc.time()-time1\n\n# Check on data\ndim(Auto)\n\n[1] 397   9\n\nAuto[1:4,] # select rows\n\n  mpg cylinders displacement horsepower weight acceleration year origin\n1  18         8          307        130   3504         12.0   70      1\n2  15         8          350        165   3693         11.5   70      1\n3  18         8          318        150   3436         11.0   70      1\n4  16         8          304        150   3433         12.0   70      1\n                       name\n1 chevrolet chevelle malibu\n2         buick skylark 320\n3        plymouth satellite\n4             amc rebel sst\n\nAuto=na.omit(Auto)\ndim(Auto) # Notice the difference?\n\n[1] 397   9\n\n# Fewer rows after NA removal\nnames(Auto)\n\n[1] \"mpg\"          \"cylinders\"    \"displacement\" \"horsepower\"   \"weight\"      \n[6] \"acceleration\" \"year\"         \"origin\"       \"name\""
  },
  {
    "objectID": "Lab02.html#load-data-from-islr-website",
    "href": "Lab02.html#load-data-from-islr-website",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "Auto=read.table(\"https://www.statlearning.com/s/Auto.data\",header=T,na.strings=\"?\")\ndim(Auto)\n\n[1] 397   9"
  },
  {
    "objectID": "Lab02.html#additional-graphical-and-numerical-summaries",
    "href": "Lab02.html#additional-graphical-and-numerical-summaries",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "# plot(cylinders, mpg)\nplot(Auto$cylinders, Auto$mpg)\n\n\n\n\n\n\n\nattach(Auto)\nplot(cylinders, mpg)\n\n\n\n\n\n\n\ncylinders=as.factor(cylinders)\nplot(cylinders, mpg)\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\")\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T)\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T,horizontal=T)\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T, xlab=\"cylinders\", ylab=\"MPG\")\n\n\n\n\n\n\n\nhist(mpg)\n\n\n\n\n\n\n\nhist(mpg,col=2)\n\n\n\n\n\n\n\nhist(mpg,col=2,breaks=15)\n\n\n\n\n\n\n\n#pairs(Auto)\npairs(~ mpg + displacement + horsepower + weight + acceleration, Auto)\n\n\n\n\n\n\n\nplot(horsepower,mpg)\n\n\n\n\n\n\n\n# identify(horsepower,mpg,name) # Interactive: point and click the dot to identify cases\nsummary(Auto)\n\n      mpg          cylinders      displacement     horsepower        weight    \n Min.   : 9.00   Min.   :3.000   Min.   : 68.0   Min.   : 46.0   Min.   :1613  \n 1st Qu.:17.50   1st Qu.:4.000   1st Qu.:104.0   1st Qu.: 75.0   1st Qu.:2223  \n Median :23.00   Median :4.000   Median :146.0   Median : 93.5   Median :2800  \n Mean   :23.52   Mean   :5.458   Mean   :193.5   Mean   :104.5   Mean   :2970  \n 3rd Qu.:29.00   3rd Qu.:8.000   3rd Qu.:262.0   3rd Qu.:126.0   3rd Qu.:3609  \n Max.   :46.60   Max.   :8.000   Max.   :455.0   Max.   :230.0   Max.   :5140  \n                                                 NA's   :5                     \n  acceleration        year           origin          name          \n Min.   : 8.00   Min.   :70.00   Min.   :1.000   Length:397        \n 1st Qu.:13.80   1st Qu.:73.00   1st Qu.:1.000   Class :character  \n Median :15.50   Median :76.00   Median :1.000   Mode  :character  \n Mean   :15.56   Mean   :75.99   Mean   :1.574                     \n 3rd Qu.:17.10   3rd Qu.:79.00   3rd Qu.:2.000                     \n Max.   :24.80   Max.   :82.00   Max.   :3.000                     \n                                                                   \n\nsummary(mpg)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   9.00   17.50   23.00   23.52   29.00   46.60"
  },
  {
    "objectID": "Lab02.html#linear-regression",
    "href": "Lab02.html#linear-regression",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "ptbu=c(\"MASS\",\"ISLR\")\ninstall.packages(ptbu, repos='http://cran.us.r-project.org')\n\nInstalling packages into 'C:/Users/bihan/AppData/Local/R/win-library/4.5'\n(as 'lib' is unspecified)\n\n\npackage 'MASS' successfully unpacked and MD5 sums checked\n\n\nWarning: cannot remove prior installation of package 'MASS'\n\n\nWarning in file.copy(savedcopy, lib, recursive = TRUE): problem copying\nC:\\Users\\bihan\\AppData\\Local\\R\\win-library\\4.5\\00LOCK\\MASS\\libs\\x64\\MASS.dll to\nC:\\Users\\bihan\\AppData\\Local\\R\\win-library\\4.5\\MASS\\libs\\x64\\MASS.dll:\nPermission denied\n\n\nWarning: restored 'MASS'\n\n\npackage 'ISLR' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n    C:\\Users\\bihan\\AppData\\Local\\Temp\\RtmpYTg9rU\\downloaded_packages\n\nlapply(ptbu, require, character.only = TRUE)\n\nLoading required package: MASS\n\n\nLoading required package: ISLR\n\n\n\nAttaching package: 'ISLR'\n\n\nThe following object is masked _by_ '.GlobalEnv':\n\n    Auto\n\n\n[[1]]\n[1] TRUE\n\n[[2]]\n[1] TRUE\n\nlibrary(MASS)\nlibrary(ISLR)\n\n# Simple Linear Regression\n\n# fix(Boston)\nnames(Boston)\n\n [1] \"crim\"    \"zn\"      \"indus\"   \"chas\"    \"nox\"     \"rm\"      \"age\"    \n [8] \"dis\"     \"rad\"     \"tax\"     \"ptratio\" \"black\"   \"lstat\"   \"medv\"   \n\n# lm.fit=lm(medv~lstat)\nattach(Boston)\nlm.fit=lm(medv~lstat,data=Boston)\nattach(Boston)\n\nThe following objects are masked from Boston (pos = 3):\n\n    age, black, chas, crim, dis, indus, lstat, medv, nox, ptratio, rad,\n    rm, tax, zn\n\nlm.fit=lm(medv~lstat)\nlm.fit\n\n\nCall:\nlm(formula = medv ~ lstat)\n\nCoefficients:\n(Intercept)        lstat  \n      34.55        -0.95  \n\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ lstat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.168  -3.990  -1.318   2.034  24.500 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 34.55384    0.56263   61.41   &lt;2e-16 ***\nlstat       -0.95005    0.03873  -24.53   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.216 on 504 degrees of freedom\nMultiple R-squared:  0.5441,    Adjusted R-squared:  0.5432 \nF-statistic: 601.6 on 1 and 504 DF,  p-value: &lt; 2.2e-16\n\nnames(lm.fit)\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\ncoef(lm.fit)\n\n(Intercept)       lstat \n 34.5538409  -0.9500494 \n\nconfint(lm.fit)\n\n                2.5 %     97.5 %\n(Intercept) 33.448457 35.6592247\nlstat       -1.026148 -0.8739505\n\npredict(lm.fit,data.frame(lstat=(c(5,10,15))), interval=\"confidence\")\n\n       fit      lwr      upr\n1 29.80359 29.00741 30.59978\n2 25.05335 24.47413 25.63256\n3 20.30310 19.73159 20.87461\n\npredict(lm.fit,data.frame(lstat=(c(5,10,15))), interval=\"prediction\")\n\n       fit       lwr      upr\n1 29.80359 17.565675 42.04151\n2 25.05335 12.827626 37.27907\n3 20.30310  8.077742 32.52846\n\n# What is the differnce between \"conference\" and \"prediction\" difference?\n# Confidence Interval: Range of average mpg for all cars with 150 horsepower. Prediction Interval: Range where one new car’s mpg with 150 horsepower might fall.\n\nplot(lstat,medv)\nabline(lm.fit)\nabline(lm.fit,lwd=3)\nabline(lm.fit,lwd=3,col=\"red\")\n\n\n\n\n\n\n\nplot(lstat,medv,col=\"red\")\n\n\n\n\n\n\n\nplot(lstat,medv,pch=16)\n\n\n\n\n\n\n\nplot(lstat,medv,pch=\"+\")\n\n\n\n\n\n\n\nplot(1:20,1:20,pch=1:20)\n\n\n\n\n\n\n\npar(mfrow=c(2,2))\nplot(lm.fit)\n\n\n\n\n\n\n\nplot(predict(lm.fit), residuals(lm.fit))\nplot(predict(lm.fit), rstudent(lm.fit))\nplot(hatvalues(lm.fit))\nwhich.max(hatvalues(lm.fit))\n\n375 \n375"
  },
  {
    "objectID": "Lab02.html#multiple-linear-regression",
    "href": "Lab02.html#multiple-linear-regression",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "lm.fit=lm(medv~lstat+age,data=Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ lstat + age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.981  -3.978  -1.283   1.968  23.158 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 33.22276    0.73085  45.458  &lt; 2e-16 ***\nlstat       -1.03207    0.04819 -21.416  &lt; 2e-16 ***\nage          0.03454    0.01223   2.826  0.00491 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.173 on 503 degrees of freedom\nMultiple R-squared:  0.5513,    Adjusted R-squared:  0.5495 \nF-statistic:   309 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\nlm.fit=lm(medv~.,data=Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ ., data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.595  -2.730  -0.518   1.777  26.199 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.646e+01  5.103e+00   7.144 3.28e-12 ***\ncrim        -1.080e-01  3.286e-02  -3.287 0.001087 ** \nzn           4.642e-02  1.373e-02   3.382 0.000778 ***\nindus        2.056e-02  6.150e-02   0.334 0.738288    \nchas         2.687e+00  8.616e-01   3.118 0.001925 ** \nnox         -1.777e+01  3.820e+00  -4.651 4.25e-06 ***\nrm           3.810e+00  4.179e-01   9.116  &lt; 2e-16 ***\nage          6.922e-04  1.321e-02   0.052 0.958229    \ndis         -1.476e+00  1.995e-01  -7.398 6.01e-13 ***\nrad          3.060e-01  6.635e-02   4.613 5.07e-06 ***\ntax         -1.233e-02  3.760e-03  -3.280 0.001112 ** \nptratio     -9.527e-01  1.308e-01  -7.283 1.31e-12 ***\nblack        9.312e-03  2.686e-03   3.467 0.000573 ***\nlstat       -5.248e-01  5.072e-02 -10.347  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.745 on 492 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7338 \nF-statistic: 108.1 on 13 and 492 DF,  p-value: &lt; 2.2e-16\n\nlibrary(car)\n\nLoading required package: carData\n\nvif(lm.fit)\n\n    crim       zn    indus     chas      nox       rm      age      dis \n1.792192 2.298758 3.991596 1.073995 4.393720 1.933744 3.100826 3.955945 \n     rad      tax  ptratio    black    lstat \n7.484496 9.008554 1.799084 1.348521 2.941491 \n\nlm.fit1=lm(medv~.-age,data=Boston)\nsummary(lm.fit1)\n\n\nCall:\nlm(formula = medv ~ . - age, data = Boston)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.6054  -2.7313  -0.5188   1.7601  26.2243 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  36.436927   5.080119   7.172 2.72e-12 ***\ncrim         -0.108006   0.032832  -3.290 0.001075 ** \nzn            0.046334   0.013613   3.404 0.000719 ***\nindus         0.020562   0.061433   0.335 0.737989    \nchas          2.689026   0.859598   3.128 0.001863 ** \nnox         -17.713540   3.679308  -4.814 1.97e-06 ***\nrm            3.814394   0.408480   9.338  &lt; 2e-16 ***\ndis          -1.478612   0.190611  -7.757 5.03e-14 ***\nrad           0.305786   0.066089   4.627 4.75e-06 ***\ntax          -0.012329   0.003755  -3.283 0.001099 ** \nptratio      -0.952211   0.130294  -7.308 1.10e-12 ***\nblack         0.009321   0.002678   3.481 0.000544 ***\nlstat        -0.523852   0.047625 -10.999  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.74 on 493 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7343 \nF-statistic: 117.3 on 12 and 493 DF,  p-value: &lt; 2.2e-16\n\nlm.fit1=update(lm.fit, ~.-age)"
  },
  {
    "objectID": "Lab02.html#non-linear-transformations-of-the-predictors",
    "href": "Lab02.html#non-linear-transformations-of-the-predictors",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "lm.fit2=lm(medv~lstat+I(lstat^2))\nsummary(lm.fit2)\n\n\nCall:\nlm(formula = medv ~ lstat + I(lstat^2))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.2834  -3.8313  -0.5295   2.3095  25.4148 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 42.862007   0.872084   49.15   &lt;2e-16 ***\nlstat       -2.332821   0.123803  -18.84   &lt;2e-16 ***\nI(lstat^2)   0.043547   0.003745   11.63   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.524 on 503 degrees of freedom\nMultiple R-squared:  0.6407,    Adjusted R-squared:  0.6393 \nF-statistic: 448.5 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\nlm.fit=lm(medv~lstat)\nanova(lm.fit,lm.fit2)\n\nAnalysis of Variance Table\n\nModel 1: medv ~ lstat\nModel 2: medv ~ lstat + I(lstat^2)\n  Res.Df   RSS Df Sum of Sq     F    Pr(&gt;F)    \n1    504 19472                                 \n2    503 15347  1    4125.1 135.2 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\npar(mfrow=c(2,2))\nplot(lm.fit2)\n\n\n\n\n\n\n\nlm.fit5=lm(medv~poly(lstat,5))\nsummary(lm.fit5)\n\n\nCall:\nlm(formula = medv ~ poly(lstat, 5))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.5433  -3.1039  -0.7052   2.0844  27.1153 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       22.5328     0.2318  97.197  &lt; 2e-16 ***\npoly(lstat, 5)1 -152.4595     5.2148 -29.236  &lt; 2e-16 ***\npoly(lstat, 5)2   64.2272     5.2148  12.316  &lt; 2e-16 ***\npoly(lstat, 5)3  -27.0511     5.2148  -5.187 3.10e-07 ***\npoly(lstat, 5)4   25.4517     5.2148   4.881 1.42e-06 ***\npoly(lstat, 5)5  -19.2524     5.2148  -3.692 0.000247 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.215 on 500 degrees of freedom\nMultiple R-squared:  0.6817,    Adjusted R-squared:  0.6785 \nF-statistic: 214.2 on 5 and 500 DF,  p-value: &lt; 2.2e-16\n\nsummary(lm(medv~log(rm),data=Boston))\n\n\nCall:\nlm(formula = medv ~ log(rm), data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-19.487  -2.875  -0.104   2.837  39.816 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -76.488      5.028  -15.21   &lt;2e-16 ***\nlog(rm)       54.055      2.739   19.73   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.915 on 504 degrees of freedom\nMultiple R-squared:  0.4358,    Adjusted R-squared:  0.4347 \nF-statistic: 389.3 on 1 and 504 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "Lab02.html#qualitative-predictors",
    "href": "Lab02.html#qualitative-predictors",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "# fix(Carseats)\nnames(Carseats)\n\n [1] \"Sales\"       \"CompPrice\"   \"Income\"      \"Advertising\" \"Population\" \n [6] \"Price\"       \"ShelveLoc\"   \"Age\"         \"Education\"   \"Urban\"      \n[11] \"US\"         \n\nlm.fit=lm(Sales~.+Income:Advertising+Price:Age,data=Carseats)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = Sales ~ . + Income:Advertising + Price:Age, data = Carseats)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9208 -0.7503  0.0177  0.6754  3.3413 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         6.5755654  1.0087470   6.519 2.22e-10 ***\nCompPrice           0.0929371  0.0041183  22.567  &lt; 2e-16 ***\nIncome              0.0108940  0.0026044   4.183 3.57e-05 ***\nAdvertising         0.0702462  0.0226091   3.107 0.002030 ** \nPopulation          0.0001592  0.0003679   0.433 0.665330    \nPrice              -0.1008064  0.0074399 -13.549  &lt; 2e-16 ***\nShelveLocGood       4.8486762  0.1528378  31.724  &lt; 2e-16 ***\nShelveLocMedium     1.9532620  0.1257682  15.531  &lt; 2e-16 ***\nAge                -0.0579466  0.0159506  -3.633 0.000318 ***\nEducation          -0.0208525  0.0196131  -1.063 0.288361    \nUrbanYes            0.1401597  0.1124019   1.247 0.213171    \nUSYes              -0.1575571  0.1489234  -1.058 0.290729    \nIncome:Advertising  0.0007510  0.0002784   2.698 0.007290 ** \nPrice:Age           0.0001068  0.0001333   0.801 0.423812    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.011 on 386 degrees of freedom\nMultiple R-squared:  0.8761,    Adjusted R-squared:  0.8719 \nF-statistic:   210 on 13 and 386 DF,  p-value: &lt; 2.2e-16\n\nattach(Carseats)\ncontrasts(ShelveLoc)\n\n       Good Medium\nBad       0      0\nGood      1      0\nMedium    0      1"
  },
  {
    "objectID": "Lab02.html#interaction-terms-including-interaction-and-single-effects",
    "href": "Lab02.html#interaction-terms-including-interaction-and-single-effects",
    "title": "EPPS 6323: Lab02 R programming basics II",
    "section": "",
    "text": "summary(lm(medv~lstat*age,data=Boston))\n\n\nCall:\nlm(formula = medv ~ lstat * age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.806  -4.045  -1.333   2.085  27.552 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 36.0885359  1.4698355  24.553  &lt; 2e-16 ***\nlstat       -1.3921168  0.1674555  -8.313 8.78e-16 ***\nage         -0.0007209  0.0198792  -0.036   0.9711    \nlstat:age    0.0041560  0.0018518   2.244   0.0252 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.149 on 502 degrees of freedom\nMultiple R-squared:  0.5557,    Adjusted R-squared:  0.5531 \nF-statistic: 209.3 on 3 and 502 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome",
    "section": "",
    "text": "Welcome\nHello! My name is Nakir, and I am a PhD candidate in Geospatial Information Sciences at The University of Texas at Dallas…\nCurrently, I am working on cassava identification using UAV imagery…\nContact:\n📧 md.nakir.ahmed.09@gmail.com\n🔗 LinkedIn"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Lab01.html",
    "href": "Lab01.html",
    "title": "EPPS 6323: Lab01 R programming basics I",
    "section": "",
    "text": "x &lt;- c(1,3,2,5)\nx\n\n[1] 1 3 2 5\n\nx = c(1,6,2)\nx\n\n[1] 1 6 2\n\ny = c(1,4,3)\n\n\n\n\n\nlength(x)  # What does length() do? \n\n[1] 3\n\n# Returns the number of elements in x\nlength(y)\n\n[1] 3\n\n\n\n\n\n\nx+y\n\n[1]  2 10  5\n\nls() # List objects in the environment\n\n[1] \"x\" \"y\"\n\nrm(x,y) # Remove objects\nls()\n\ncharacter(0)\n\nrm(list=ls()) # Danger! What does this do?  Not recommended!\n# Removes all objects from the environment\n\n\n\n\n\n?matrix # Shows help file for matrix function\n\nstarting httpd help server ... done\n\nx=matrix(data=c(1,2,3,4), nrow=2, ncol=2) # Create a 2x2 matrix object\nx\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\nx=matrix(c(1,2,3,4),2,2)\nmatrix(c(1,2,3,4),2,2,byrow=T) # What about byrow=F?\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n\n# Fills matrix by row\n\nsqrt(x) # What does x look like?\n\n         [,1]     [,2]\n[1,] 1.000000 1.732051\n[2,] 1.414214 2.000000\n\n# Element-wise square root\nx\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\nx^2\n\n     [,1] [,2]\n[1,]    1    9\n[2,]    4   16\n\nx=rnorm(50) # Generate a vector of 50 numbers using the rnorm() function\n\ny=x+rnorm(50,mean=50,sd=.1) # What does rnorm(50,mean=50,sd=.1) generate?\n# Adds random noise centered at 50\ncor(x,y) # Correlation of x and y\n\n[1] 0.9918287\n\nset.seed(1303) # Set the seed for Random Number Generator (RNG) to generate values that are reproducible.\nrnorm(50)\n\n [1] -1.1439763145  1.3421293656  2.1853904757  0.5363925179  0.0631929665\n [6]  0.5022344825 -0.0004167247  0.5658198405 -0.5725226890 -1.1102250073\n[11] -0.0486871234 -0.6956562176  0.8289174803  0.2066528551 -0.2356745091\n[16] -0.5563104914 -0.3647543571  0.8623550343 -0.6307715354  0.3136021252\n[21] -0.9314953177  0.8238676185  0.5233707021  0.7069214120  0.4202043256\n[26] -0.2690521547 -1.5103172999 -0.6902124766 -0.1434719524 -1.0135274099\n[31]  1.5732737361  0.0127465055  0.8726470499  0.4220661905 -0.0188157917\n[36]  2.6157489689 -0.6931401748 -0.2663217810 -0.7206364412  1.3677342065\n[41]  0.2640073322  0.6321868074 -1.3306509858  0.0268888182  1.0406363208\n[46]  1.3120237985 -0.0300020767 -0.2500257125  0.0234144857  1.6598706557\n\nset.seed(3) # Try different seeds?\ny=rnorm(100)\n\n\n\n\n\nmean(y)\n\n[1] 0.01103557\n\nvar(y)\n\n[1] 0.7328675\n\nsqrt(var(y))\n\n[1] 0.8560768\n\nsd(y)\n\n[1] 0.8560768\n\n\n\n\n\n\nx=rnorm(100)\ny=rnorm(100)\nplot(x,y)\n\n\n\n\n\n\n\nplot(x,y, pch=20, col = \"firebrick\") # Scatterplot for two numeric variables by default\n\n\n\n\n\n\n\nplot(x,y, pch=20, col = \"steelblue\",xlab=\"this is the x-axis\",ylab=\"this is the y-axis\",main=\"Plot of X vs Y\") # Add labels\n\n\n\n\n\n\n\npdf(\"Figure01.pdf\") # Save as pdf, add a path or it will be stored on the project directory\nplot(x,y,pch=20, col=\"forestgreen\") # Try different colors?\ndev.off() # Close the file using the dev.off function\n\npng \n  2 \n\nx=seq(1,10) # Same as x=c(1:10)\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx=1:10\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx=seq(-pi,pi,length=50)\ny=x\n\n\n# Generate data\nx = rnorm(100)\ny = rnorm(100)\n\n# Display plot in RStudio viewer\nplot(x, y, pch = 20, col = \"green\",\n     xlab = \"this is the x-axis\",\n     ylab = \"this is the y-axis\",\n     main = \"Plot of X vs Y\")\n\n\n\n\n\n\n\n# Save same plot to a new PDF file (Figure02.pdf)\npdf(\"Figure02.pdf\")\nplot(x, y, pch = 20, col = \"black\",\n     xlab = \"this is the x-axis\",\n     ylab = \"this is the y-axis\",\n     main = \"Plot of X vs Y\")\ndev.off()\n\npng \n  2 \n\n# Sequence examples\nx = seq(1, 10)\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx = 1:10\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx = seq(-pi, pi, length = 50)\ny = x"
  },
  {
    "objectID": "Lab01.html#create-object-using-the-assignment-operator--",
    "href": "Lab01.html#create-object-using-the-assignment-operator--",
    "title": "EPPS 6323: Lab01 R programming basics I",
    "section": "",
    "text": "x &lt;- c(1,3,2,5)\nx\n\n[1] 1 3 2 5\n\nx = c(1,6,2)\nx\n\n[1] 1 6 2\n\ny = c(1,4,3)"
  },
  {
    "objectID": "Lab01.html#using-function",
    "href": "Lab01.html#using-function",
    "title": "EPPS 6323: Lab01 R programming basics I",
    "section": "",
    "text": "length(x)  # What does length() do? \n\n[1] 3\n\n# Returns the number of elements in x\nlength(y)\n\n[1] 3"
  },
  {
    "objectID": "Lab01.html#using---operators",
    "href": "Lab01.html#using---operators",
    "title": "EPPS 6323: Lab01 R programming basics I",
    "section": "",
    "text": "x+y\n\n[1]  2 10  5\n\nls() # List objects in the environment\n\n[1] \"x\" \"y\"\n\nrm(x,y) # Remove objects\nls()\n\ncharacter(0)\n\nrm(list=ls()) # Danger! What does this do?  Not recommended!\n# Removes all objects from the environment"
  },
  {
    "objectID": "Lab01.html#matrix-operations",
    "href": "Lab01.html#matrix-operations",
    "title": "EPPS 6323: Lab01 R programming basics I",
    "section": "",
    "text": "?matrix # Shows help file for matrix function\n\nstarting httpd help server ... done\n\nx=matrix(data=c(1,2,3,4), nrow=2, ncol=2) # Create a 2x2 matrix object\nx\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\nx=matrix(c(1,2,3,4),2,2)\nmatrix(c(1,2,3,4),2,2,byrow=T) # What about byrow=F?\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n\n# Fills matrix by row\n\nsqrt(x) # What does x look like?\n\n         [,1]     [,2]\n[1,] 1.000000 1.732051\n[2,] 1.414214 2.000000\n\n# Element-wise square root\nx\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\nx^2\n\n     [,1] [,2]\n[1,]    1    9\n[2,]    4   16\n\nx=rnorm(50) # Generate a vector of 50 numbers using the rnorm() function\n\ny=x+rnorm(50,mean=50,sd=.1) # What does rnorm(50,mean=50,sd=.1) generate?\n# Adds random noise centered at 50\ncor(x,y) # Correlation of x and y\n\n[1] 0.9918287\n\nset.seed(1303) # Set the seed for Random Number Generator (RNG) to generate values that are reproducible.\nrnorm(50)\n\n [1] -1.1439763145  1.3421293656  2.1853904757  0.5363925179  0.0631929665\n [6]  0.5022344825 -0.0004167247  0.5658198405 -0.5725226890 -1.1102250073\n[11] -0.0486871234 -0.6956562176  0.8289174803  0.2066528551 -0.2356745091\n[16] -0.5563104914 -0.3647543571  0.8623550343 -0.6307715354  0.3136021252\n[21] -0.9314953177  0.8238676185  0.5233707021  0.7069214120  0.4202043256\n[26] -0.2690521547 -1.5103172999 -0.6902124766 -0.1434719524 -1.0135274099\n[31]  1.5732737361  0.0127465055  0.8726470499  0.4220661905 -0.0188157917\n[36]  2.6157489689 -0.6931401748 -0.2663217810 -0.7206364412  1.3677342065\n[41]  0.2640073322  0.6321868074 -1.3306509858  0.0268888182  1.0406363208\n[46]  1.3120237985 -0.0300020767 -0.2500257125  0.0234144857  1.6598706557\n\nset.seed(3) # Try different seeds?\ny=rnorm(100)"
  },
  {
    "objectID": "Lab01.html#simple-descriptive-statistics-base",
    "href": "Lab01.html#simple-descriptive-statistics-base",
    "title": "EPPS 6323: Lab01 R programming basics I",
    "section": "",
    "text": "mean(y)\n\n[1] 0.01103557\n\nvar(y)\n\n[1] 0.7328675\n\nsqrt(var(y))\n\n[1] 0.8560768\n\nsd(y)\n\n[1] 0.8560768"
  },
  {
    "objectID": "Lab01.html#visualization-using-r-graphics-without-packages",
    "href": "Lab01.html#visualization-using-r-graphics-without-packages",
    "title": "EPPS 6323: Lab01 R programming basics I",
    "section": "",
    "text": "x=rnorm(100)\ny=rnorm(100)\nplot(x,y)\n\n\n\n\n\n\n\nplot(x,y, pch=20, col = \"firebrick\") # Scatterplot for two numeric variables by default\n\n\n\n\n\n\n\nplot(x,y, pch=20, col = \"steelblue\",xlab=\"this is the x-axis\",ylab=\"this is the y-axis\",main=\"Plot of X vs Y\") # Add labels\n\n\n\n\n\n\n\npdf(\"Figure01.pdf\") # Save as pdf, add a path or it will be stored on the project directory\nplot(x,y,pch=20, col=\"forestgreen\") # Try different colors?\ndev.off() # Close the file using the dev.off function\n\npng \n  2 \n\nx=seq(1,10) # Same as x=c(1:10)\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx=1:10\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx=seq(-pi,pi,length=50)\ny=x\n\n\n# Generate data\nx = rnorm(100)\ny = rnorm(100)\n\n# Display plot in RStudio viewer\nplot(x, y, pch = 20, col = \"green\",\n     xlab = \"this is the x-axis\",\n     ylab = \"this is the y-axis\",\n     main = \"Plot of X vs Y\")\n\n\n\n\n\n\n\n# Save same plot to a new PDF file (Figure02.pdf)\npdf(\"Figure02.pdf\")\nplot(x, y, pch = 20, col = \"black\",\n     xlab = \"this is the x-axis\",\n     ylab = \"this is the y-axis\",\n     main = \"Plot of X vs Y\")\ndev.off()\n\npng \n  2 \n\n# Sequence examples\nx = seq(1, 10)\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx = 1:10\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx = seq(-pi, pi, length = 50)\ny = x"
  },
  {
    "objectID": "KM_Assignment05_Nakir.html",
    "href": "KM_Assignment05_Nakir.html",
    "title": "Improving Text Classification with Penalized Logistic Regression",
    "section": "",
    "text": "This analysis demonstrates how to improve prediction performance for a text classification task using TF-IDF features and a penalized logistic regression model (elastic net) in R."
  },
  {
    "objectID": "KM_Assignment05_Nakir.html#goal",
    "href": "KM_Assignment05_Nakir.html#goal",
    "title": "Improving Text Classification with Penalized Logistic Regression",
    "section": "",
    "text": "This analysis demonstrates how to improve prediction performance for a text classification task using TF-IDF features and a penalized logistic regression model (elastic net) in R."
  },
  {
    "objectID": "KM_Assignment05_Nakir.html#step-1-load-required-libraries",
    "href": "KM_Assignment05_Nakir.html#step-1-load-required-libraries",
    "title": "Improving Text Classification with Penalized Logistic Regression",
    "section": "📥 Step 1: Load Required Libraries",
    "text": "📥 Step 1: Load Required Libraries\nWe load tidyverse tools for data manipulation, tidymodels for modeling, and textrecipes for text preprocessing.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.3.0 ──\n✔ broom        1.0.8     ✔ rsample      1.3.0\n✔ dials        1.4.0     ✔ tune         1.3.0\n✔ infer        1.0.8     ✔ workflows    1.2.0\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.3.1     ✔ yardstick    1.3.2\n✔ recipes      1.3.0     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n\nlibrary(textrecipes)\nlibrary(workflows)\nlibrary(stopwords)"
  },
  {
    "objectID": "KM_Assignment05_Nakir.html#step-2-load-and-prepare-the-data",
    "href": "KM_Assignment05_Nakir.html#step-2-load-and-prepare-the-data",
    "title": "Improving Text Classification with Penalized Logistic Regression",
    "section": "📄 Step 2: Load and Prepare the Data",
    "text": "📄 Step 2: Load and Prepare the Data\nWe load a labeled corpus of 200 short documents and convert the target label to a factor for classification. We split the data 70/30 for training and testing.\n\ndata &lt;- read_csv(\"https://raw.githubusercontent.com/datageneration/knowledgemining/refs/heads/master/data/km_sample_corpus_200.csv\") %&gt;%\n  mutate(label = factor(label))\n\nRows: 200 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): text, label\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nset.seed(123)\nsplit &lt;- initial_split(data, prop = 0.7, strata = label)\ntrain_data &lt;- training(split)\ntest_data  &lt;- testing(split)"
  },
  {
    "objectID": "KM_Assignment05_Nakir.html#step-3-text-preprocessing-with-tf-idf",
    "href": "KM_Assignment05_Nakir.html#step-3-text-preprocessing-with-tf-idf",
    "title": "Improving Text Classification with Penalized Logistic Regression",
    "section": "✂️ Step 3: Text Preprocessing with TF-IDF",
    "text": "✂️ Step 3: Text Preprocessing with TF-IDF\nWe create a recipe to: - Tokenize the text - Remove stopwords - Keep the 500 most frequent tokens - Convert tokens into TF-IDF weighted features\nThis enriches the feature set and improves classification performance.\n\nrec &lt;- recipe(label ~ text, data = train_data) %&gt;%\n  step_tokenize(text) %&gt;%\n  step_stopwords(text) %&gt;%\n  step_tokenfilter(text, max_tokens = 500) %&gt;%\n  step_tfidf(text)"
  },
  {
    "objectID": "KM_Assignment05_Nakir.html#step-4-define-penalized-logistic-regression-model",
    "href": "KM_Assignment05_Nakir.html#step-4-define-penalized-logistic-regression-model",
    "title": "Improving Text Classification with Penalized Logistic Regression",
    "section": "🌲 Step 4: Define Penalized Logistic Regression Model",
    "text": "🌲 Step 4: Define Penalized Logistic Regression Model\nWe use multinom_reg() from parsnip with glmnet engine to perform logistic regression with elastic net regularization. The penalty (λ) and mixture (α) parameters will be tuned.\n\nlr_spec &lt;- multinom_reg(penalty = tune(), mixture = tune()) %&gt;%\n  set_engine(\"glmnet\") %&gt;%\n  set_mode(\"classification\")"
  },
  {
    "objectID": "KM_Assignment05_Nakir.html#step-5-workflow-setup",
    "href": "KM_Assignment05_Nakir.html#step-5-workflow-setup",
    "title": "Improving Text Classification with Penalized Logistic Regression",
    "section": "🔧 Step 5: Workflow Setup",
    "text": "🔧 Step 5: Workflow Setup\nWe combine the recipe and model into a single workflow, which simplifies the training and tuning process.\n\nlibrary(workflows)\nwf &lt;- workflow() %&gt;%\n  add_recipe(rec) %&gt;%\n  add_model(lr_spec)"
  },
  {
    "objectID": "KM_Assignment05_Nakir.html#step-6-cross-validation",
    "href": "KM_Assignment05_Nakir.html#step-6-cross-validation",
    "title": "Improving Text Classification with Penalized Logistic Regression",
    "section": "🔁 Step 6: Cross-Validation",
    "text": "🔁 Step 6: Cross-Validation\nWe perform 5-fold cross-validation to robustly estimate model performance during tuning.\n\nset.seed(123)\ncv_folds &lt;- vfold_cv(train_data, v = 5, strata = label)"
  },
  {
    "objectID": "KM_Assignment05_Nakir.html#step-7-define-hyperparameter-grid",
    "href": "KM_Assignment05_Nakir.html#step-7-define-hyperparameter-grid",
    "title": "Improving Text Classification with Penalized Logistic Regression",
    "section": "🎯 Step 7: Define Hyperparameter Grid",
    "text": "🎯 Step 7: Define Hyperparameter Grid\nWe define a grid of 25 combinations by varying: - penalty: strength of regularization (log scale from 10^-4 to 1) - mixture: 0 = ridge, 1 = lasso, in between = elastic net\n\nlr_grid &lt;- grid_regular(\n  penalty(range = c(-4, 0)),\n  mixture(range = c(0, 1)),\n  levels = 5\n)"
  },
  {
    "objectID": "KM_Assignment05_Nakir.html#step-8-tune-the-model",
    "href": "KM_Assignment05_Nakir.html#step-8-tune-the-model",
    "title": "Improving Text Classification with Penalized Logistic Regression",
    "section": "🔍 Step 8: Tune the Model",
    "text": "🔍 Step 8: Tune the Model\nWe tune the model using the workflow and grid defined above. Accuracy and Kappa are used as metrics.\n\nset.seed(123)\ntune_results &lt;- tune_grid(\n  wf,\n  resamples = cv_folds,\n  grid = lr_grid,\n  metrics = metric_set(accuracy, kap)\n)\n\n→ A | warning: max_tokens was set to 500, but only 131 was available and selected.\n\n\nThere were issues with some computations   A: x1\n\n\n→ B | warning: max_tokens was set to 500, but only 133 was available and selected.\n\n\nThere were issues with some computations   A: x1\n→ C | warning: max_tokens was set to 500, but only 137 was available and selected.\nThere were issues with some computations   A: x1\nThere were issues with some computations   A: x1   B: x1   C: x1\n→ D | warning: max_tokens was set to 500, but only 132 was available and selected.\nThere were issues with some computations   A: x1   B: x1   C: x1\nThere were issues with some computations   A: x1   B: x1   C: x2   D: x1"
  },
  {
    "objectID": "KM_Assignment05_Nakir.html#step-9-select-best-model",
    "href": "KM_Assignment05_Nakir.html#step-9-select-best-model",
    "title": "Improving Text Classification with Penalized Logistic Regression",
    "section": "✅ Step 9: Select Best Model",
    "text": "✅ Step 9: Select Best Model\nWe select the best combination of hyperparameters based on highest accuracy.\n\nbest_params &lt;- select_best(tune_results, metric = \"accuracy\")"
  },
  {
    "objectID": "KM_Assignment05_Nakir.html#step-10-finalize-and-train-model",
    "href": "KM_Assignment05_Nakir.html#step-10-finalize-and-train-model",
    "title": "Improving Text Classification with Penalized Logistic Regression",
    "section": "🏗️ Step 10: Finalize and Train Model",
    "text": "🏗️ Step 10: Finalize and Train Model\nWe finalize the workflow with the best hyperparameters and fit the model on the full training dataset.\n\nfinal_wf &lt;- finalize_workflow(wf, best_params)\nfinal_fit &lt;- final_wf %&gt;% fit(data = train_data)\n\nWarning: max_tokens was set to 500, but only 137 was available and selected."
  },
  {
    "objectID": "KM_Assignment05_Nakir.html#step-11-predict-and-evaluate-on-test-set",
    "href": "KM_Assignment05_Nakir.html#step-11-predict-and-evaluate-on-test-set",
    "title": "Improving Text Classification with Penalized Logistic Regression",
    "section": "📈 Step 11: Predict and Evaluate on Test Set",
    "text": "📈 Step 11: Predict and Evaluate on Test Set\nWe generate predictions and evaluate them using accuracy and Kappa score.\n\nfinal_preds &lt;- predict(final_fit, new_data = test_data) %&gt;%\n  bind_cols(test_data)\n\nfinal_metrics &lt;- metric_set(accuracy, kap)(final_preds, truth = label, estimate = .pred_class)\nfinal_metrics\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy multiclass     0.95 \n2 kap      multiclass     0.944"
  },
  {
    "objectID": "KM_Assignment05_Nakir.html#step-12-confusion-matrix",
    "href": "KM_Assignment05_Nakir.html#step-12-confusion-matrix",
    "title": "Improving Text Classification with Penalized Logistic Regression",
    "section": "📊 Step 12: Confusion Matrix",
    "text": "📊 Step 12: Confusion Matrix\nWe visualize model performance using a confusion matrix to understand misclassifications.\n\nconf_mat(final_preds, truth = label, estimate = .pred_class)\n\n               Truth\nPrediction      Culture Education Entertainment Environment Finance Health\n  Culture             6         0             0           0       0      0\n  Education           0         6             0           0       0      0\n  Entertainment       0         0             6           0       0      0\n  Environment         0         0             0           6       0      0\n  Finance             0         0             0           0       6      0\n  Health              0         0             0           0       0      6\n  Politics            0         0             0           0       0      0\n  Sports              0         0             0           0       0      0\n  Technology          0         0             0           0       0      0\n  Travel              0         0             0           0       0      0\n               Truth\nPrediction      Politics Sports Technology Travel\n  Culture              0      0          0      0\n  Education            3      0          0      0\n  Entertainment        0      0          0      0\n  Environment          0      0          0      0\n  Finance              0      0          0      0\n  Health               0      0          0      0\n  Politics             3      0          0      0\n  Sports               0      6          0      0\n  Technology           0      0          6      0\n  Travel               0      0          0      6"
  },
  {
    "objectID": "KM_Assignment05_Nakir.html#summary-how-we-improved-prediction",
    "href": "KM_Assignment05_Nakir.html#summary-how-we-improved-prediction",
    "title": "Improving Text Classification with Penalized Logistic Regression",
    "section": "✅ Summary: How We Improved Prediction",
    "text": "✅ Summary: How We Improved Prediction\nCompared to the earlier Random Forest approach, this model: - Used a penalized logistic regression, which is better suited for sparse high-dimensional data like TF-IDF - Tuned regularization parameters, improving generalization - Used more features (500 tokens vs. 100), capturing more context\nThis workflow is more interpretable, efficient, and predictive for TF-IDF-based text classification."
  },
  {
    "objectID": "KM_Assignment04_Nakir.html#exercise-try-downloading-118th-congress-congressional-hearings-in-committee-on-foreign-affairs",
    "href": "KM_Assignment04_Nakir.html#exercise-try-downloading-118th-congress-congressional-hearings-in-committee-on-foreign-affairs",
    "title": "Assignment 03",
    "section": "6. Exercise: Try downloading 118th Congress Congressional Hearings in Committee on Foreign Affairs?",
    "text": "6. Exercise: Try downloading 118th Congress Congressional Hearings in Committee on Foreign Affairs?\nWe explore U.S. government documents containing “Foreign Affairs” in the title or teaser. Since there’s no dedicated congress field, we inspect packageId and apply flexible filtering.\n\nlibrary(jsonlite)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n# Read JSON and extract documents\ngf_list1 &lt;- read_json(\"https://github.com/datageneration/datamethods/raw/refs/heads/master/webdata/govinfo-search-results-2024-10-13T07_18_29.json\")\ngovfiles3 &lt;- bind_rows(gf_list1$resultSet)\n\n# Filter for Foreign Affairs\nforeign_affairs_docs &lt;- govfiles3 %&gt;%\n  filter(grepl(\"Foreign Affairs\", title, ignore.case = TRUE))\n\nhead(foreign_affairs_docs$title)\n\n[1] \"A bill to establish the Office of Press Freedom, to create press freedom curriculum at the National Foreign Affairs Training Center, and for other purposes; to the Committee on Foreign Relations.\""
  },
  {
    "objectID": "KM_Assignment04_Nakir.html#create-a-corpus-using-government-documents-selected-from-the-govinfo.gov-website",
    "href": "KM_Assignment04_Nakir.html#create-a-corpus-using-government-documents-selected-from-the-govinfo.gov-website",
    "title": "Assignment 03",
    "section": "7. Create a corpus using government documents selected from the govinfo.gov website",
    "text": "7. Create a corpus using government documents selected from the govinfo.gov website\nTo avoid replicating earlier examples, we use a different Foreign Affairs-related document titled:\n\n“A bill to establish the Office of Press Freedom, to create press freedom curriculum at the National Foreign Affairs Training Center…”\n\nThis document is selected from the govinfo.gov dataset and will be downloaded and processed into a corpus.\n\nlibrary(jsonlite)\nlibrary(pdftools)\n\nUsing poppler version 25.02.0\n\nlibrary(quanteda)\n\n# Load metadata from JSON\ngf_list1 &lt;- read_json(\"https://github.com/datageneration/datamethods/raw/refs/heads/master/webdata/govinfo-search-results-2024-10-13T07_18_29.json\")\ngovfiles3 &lt;- bind_rows(gf_list1$resultSet)\n\n# Find document about Press Freedom\ndoc_index &lt;- which(grepl(\"Press Freedom\", govfiles3$title))\nurl &lt;- govfiles3$pdfLink[doc_index]\nid &lt;- govfiles3$index[doc_index]\n\n# Set your actual directory path\nsave_dir &lt;- \"D:/UTD/Courses/Spring 2025/EPPS 6323 Knowledge Mining/assignments/KM_Assignment04/\"\n\n# Create directory if it doesn't exist\ndir.create(save_dir, recursive = TRUE, showWarnings = FALSE)\n\n# Use only the first matching document (in case there are multiple)\ndestfile &lt;- paste0(save_dir, \"govfiles_\", id[1], \".pdf\")\n\n# Download PDF\ndownload.file(url[1], destfile, mode = \"wb\")\n\n# Extract text and build corpus\ntxt &lt;- pdf_text(destfile)\nfull_text &lt;- paste(txt, collapse = \"\\n\")\ncorp &lt;- corpus(full_text)\n\n# Tokenize and plot\ndfm_doc &lt;- corp %&gt;%\n  tokens(remove_punct = TRUE) %&gt;%\n  tokens_remove(stopwords(\"english\")) %&gt;%\n  dfm() %&gt;%\n  dfm_trim(min_termfreq = 3)\n\ntextplot_wordcloud(dfm_doc)"
  },
  {
    "objectID": "KM_Assignment04_Nakir.html#additional-analysis",
    "href": "KM_Assignment04_Nakir.html#additional-analysis",
    "title": "Assignment 03",
    "section": "Additional Analysis",
    "text": "Additional Analysis\nTo deepen the corpus analysis beyond the word cloud, we include:\n\n🔹 Top Words Bar Plot\nDisplays the 20 most frequent terms in the document.\n\ntopwords &lt;- topfeatures(dfm_doc, 20)\nbarplot(topwords, las = 2, col = \"skyblue\", main = \"Top Terms in the Press Freedom Bill\")\n\n\n\n\n\n\n\n\n\n\n🔹 Document Summary\nGeneral metadata about the corpus object.\n\nsummary(corp)\n\nCorpus consisting of 1 document, showing 1 document:\n\n  Text Types Tokens Sentences\n text1  2256  10811        43"
  },
  {
    "objectID": "KM_Assignment02_Nakir.html",
    "href": "KM_Assignment02_Nakir.html",
    "title": "Assignment 02",
    "section": "",
    "text": "Please check the lab section to see Lab 01 and Lab 02."
  },
  {
    "objectID": "KM_Assignment02_Nakir.html#lab-01-and-lab-02",
    "href": "KM_Assignment02_Nakir.html#lab-01-and-lab-02",
    "title": "Assignment 02",
    "section": "",
    "text": "Please check the lab section to see Lab 01 and Lab 02."
  },
  {
    "objectID": "KM_Assignment02_Nakir.html#load-and-explore-the-teds2016-dataset",
    "href": "KM_Assignment02_Nakir.html#load-and-explore-the-teds2016-dataset",
    "title": "Assignment 02",
    "section": "2) Load and Explore the TEDS2016 Dataset",
    "text": "2) Load and Explore the TEDS2016 Dataset\n\nlibrary(haven)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\nlibrary(car)\n\nLoading required package: carData\n\n\n\nAttaching package: 'car'\n\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\nTEDS_2016 &lt;- read_stata(\"https://github.com/datageneration/home/blob/master/DataProgramming/data/TEDS_2016.dta?raw=true\")\nhead(TEDS_2016)\n\n# A tibble: 6 × 54\n  District      Sex     Age     Edu     Arear   Career  Career8 Ethnic  Party   \n  &lt;dbl+lbl&gt;     &lt;dbl+l&gt; &lt;dbl+l&gt; &lt;dbl+l&gt; &lt;dbl+l&gt; &lt;dbl+l&gt; &lt;dbl+l&gt; &lt;dbl+l&gt; &lt;dbl+lb&gt;\n1 201 [Yi Lan … 2 [Fem… 4 [50-… 4 [Col… 1 [Tai… 1 [Hig… 1 [Civ… 1 [Tai… 25 [Neu…\n2 201 [Yi Lan … 2 [Fem… 2 [30-… 5 [Abo… 1 [Tai… 2 [Low… 3 [CLE… 2 [Bot… 25 [Neu…\n3 201 [Yi Lan … 1 [Mal… 5 [Abo… 5 [Abo… 1 [Tai… 1 [Hig… 1 [Civ… 2 [Bot…  3 [Lea…\n4 201 [Yi Lan … 1 [Mal… 4 [50-… 2 [Jun… 1 [Tai… 4 [WOR… 4 [Lab… 1 [Tai… 25 [Neu…\n5 201 [Yi Lan … 2 [Fem… 5 [Abo… 1 [Bel… 1 [Tai… 3 [FAR… 5 [FAR… 9 [Nor… 25 [Neu…\n6 201 [Yi Lan … 2 [Fem… 5 [Abo… 2 [Jun… 1 [Tai… 2 [Low… 7 [Hou… 1 [Tai…  6 [Som…\n# ℹ 45 more variables: PartyID &lt;dbl+lbl&gt;, Tondu &lt;dbl+lbl&gt;, Tondu3 &lt;dbl+lbl&gt;,\n#   nI2 &lt;dbl+lbl&gt;, votetsai &lt;dbl&gt;, green &lt;dbl&gt;, votetsai_nm &lt;dbl&gt;,\n#   votetsai_all &lt;dbl&gt;, Independence &lt;dbl&gt;, Unification &lt;dbl&gt;, sq &lt;dbl&gt;,\n#   Taiwanese &lt;dbl&gt;, edu &lt;dbl&gt;, female &lt;dbl&gt;, whitecollar &lt;dbl&gt;,\n#   lowincome &lt;dbl&gt;, income &lt;dbl&gt;, income_nm &lt;dbl&gt;, age &lt;dbl&gt;, KMT &lt;dbl&gt;,\n#   DPP &lt;dbl&gt;, npp &lt;dbl&gt;, noparty &lt;dbl&gt;, pfp &lt;dbl&gt;, South &lt;dbl&gt;, north &lt;dbl&gt;,\n#   Minnan_father &lt;dbl&gt;, Mainland_father &lt;dbl&gt;, Econ_worse &lt;dbl&gt;, …\n\nstr(TEDS_2016)\n\ntibble [1,690 × 54] (S3: tbl_df/tbl/data.frame)\n $ District       : dbl+lbl [1:1690] 201, 201, 201, 201, 201, 201, 201, 201, 201, 201, 201...\n   ..@ label       : chr \"District\"\n   ..@ format.stata: chr \"%10.0g\"\n   ..@ labels      : Named num [1:73] 201 401 501 502 701 702 703 704 801 802 ...\n   .. ..- attr(*, \"names\")= chr [1:73] \"Yi Lan County Single District\" \"Hsinchu County Single District\" \"Miaoli County 1st District\" \"Miaoli County 2nd District\" ...\n $ Sex            : dbl+lbl [1:1690] 2, 2, 1, 1, 2, 2, 1, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 1,...\n   ..@ label       : chr \"Sex\"\n   ..@ format.stata: chr \"%10.0g\"\n   ..@ labels      : Named num [1:2] 1 2\n   .. ..- attr(*, \"names\")= chr [1:2] \"Male\" \"Female\"\n $ Age            : dbl+lbl [1:1690] 4, 2, 5, 4, 5, 5, 5, 4, 5, 4, 5, 1, 5, 3, 4, 5, 4, 5,...\n   ..@ label       : chr \"Age\"\n   ..@ format.stata: chr \"%10.0g\"\n   ..@ labels      : Named num [1:5] 1 2 3 4 5\n   .. ..- attr(*, \"names\")= chr [1:5] \"20-29\" \"30-39\" \"40-49\" \"50-59\" ...\n $ Edu            : dbl+lbl [1:1690] 4, 5, 5, 2, 1, 2, 1, 5, 1, 1, 1, 2, 1, 5, 5, 1, 3, 4,...\n   ..@ label       : chr \"Education\"\n   ..@ format.stata: chr \"%10.0g\"\n   ..@ labels      : Named num [1:6] 1 2 3 4 5 9\n   .. ..- attr(*, \"names\")= chr [1:6] \"Below elementary school\" \"Junior high school\" \"Senior high school\" \"College\" ...\n $ Arear          : dbl+lbl [1:1690] 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...\n   ..@ label       : chr \"Area\"\n   ..@ format.stata: chr \"%10.0g\"\n   ..@ labels      : Named num [1:6] 1 2 3 4 5 6\n   .. ..- attr(*, \"names\")= chr [1:6] \"Taipei, New Taipei, Keelung and Yi Lan\" \"Taoyuan, Hsinchu and Miaoli\" \"Taichung, Changhua and Nantou\" \"Yunlin, Chiayi and Tainan\" ...\n $ Career         : dbl+lbl [1:1690] 1, 2, 1, 4, 3, 2, 4, 1, 4, 3, 3, 5, 5, 4, 1, 5, 2, 2,...\n   ..@ label       : chr \"Occupations5\"\n   ..@ format.stata: chr \"%10.0g\"\n   ..@ labels      : Named num [1:5] 1 2 3 4 5\n   .. ..- attr(*, \"names\")= chr [1:5] \"Hight-class WHITE COLLAR\" \"Low-class WHITE COLLAR\" \"FARMER\" \"WORKER\" ...\n $ Career8        : dbl+lbl [1:1690] 1, 3, 1, 4, 5, 7, 4, 2, 4, 5, 5, 7, 7, 7, 2, 7, 3, 1,...\n   ..@ label       : chr \"Occupation8\"\n   ..@ format.stata: chr \"%10.0g\"\n   ..@ labels      : Named num [1:8] 1 2 3 4 5 6 7 8\n   .. ..- attr(*, \"names\")= chr [1:8] \"Civil servants\" \"Managers and  Professionals (priv.)\" \"CLERKS (priv.)\" \"Labor (priv.)\" ...\n $ Ethnic         : dbl+lbl [1:1690] 1, 2, 2, 1, 9, 1, 2, 1, 1, 2, 1, 1, 2, 1, 2, 9, 2, 2,...\n   ..@ label       : chr \"Ethnic\"\n   ..@ format.stata: chr \"%10.0g\"\n   ..@ labels      : Named num [1:4] 1 2 3 9\n   .. ..- attr(*, \"names\")= chr [1:4] \"Taiwanese\" \"Both\" \"Chinese\" \"Noresponse\"\n $ Party          : dbl+lbl [1:1690] 25, 25,  3, 25, 25,  6, 25, 24, 25, 25,  6,  5, 25,  ...\n   ..@ label       : chr \"Party Preference\"\n   ..@ format.stata: chr \"%10.0g\"\n   ..@ labels      : Named num [1:26] 1 2 3 4 5 6 7 8 9 10 ...\n   .. ..- attr(*, \"names\")= chr [1:26] \"Strongly support KMT\" \"Somewhat support KMT\" \"Lean to KMT\" \"Somewhat lean to KMT\" ...\n $ PartyID        : dbl+lbl [1:1690] 9, 9, 1, 9, 9, 2, 9, 6, 9, 9, 2, 2, 9, 1, 1, 9, 9, 9,...\n   ..@ label       : chr \"Party Identification\"\n   ..@ format.stata: chr \"%10.0g\"\n   ..@ labels      : Named num [1:7] 1 2 3 4 5 6 9\n   .. ..- attr(*, \"names\")= chr [1:7] \"KMT\" \"DPP\" \"NP\" \"PFP\" ...\n $ Tondu          : dbl+lbl [1:1690] 3, 5, 3, 5, 9, 4, 9, 6, 9, 9, 5, 5, 9, 5, 4, 9, 9, 4,...\n   ..@ label       : chr \"Position on unification and independence\"\n   ..@ format.stata: chr \"%10.0g\"\n   ..@ labels      : Named num [1:7] 1 2 3 4 5 6 9\n   .. ..- attr(*, \"names\")= chr [1:7] \"Immediate unification\" \"Maintain the status quo,move toward unification\" \"Maintain the status quo, decide either unification or independence\" \"Maintain the status quo forever\" ...\n $ Tondu3         : dbl+lbl [1:1690] 2, 3, 2, 3, 9, 2, 9, 3, 9, 9, 3, 3, 9, 3, 2, 9, 9, 2,...\n   ..@ label       : chr \"3 categories of TONDU\"\n   ..@ format.stata: chr \"%10.0g\"\n   ..@ labels      : Named num [1:4] 1 2 3 9\n   .. ..- attr(*, \"names\")= chr [1:4] \"Unification\" \"Maintain the status quo\" \"Independence\" \"Nonresponse\"\n $ nI2            : dbl+lbl [1:1690]  3, 98, 98,  3, 98, 98, 98,  3, 98,  1,  2, 98, 98,  ...\n   ..@ label       : chr \"Who is the current the premier of our country?\"\n   ..@ format.stata: chr \"%10.0g\"\n   ..@ labels      : Named num [1:5] 1 2 3 95 98\n   .. ..- attr(*, \"names\")= chr [1:5] \"Correct\" \"Incorrect\" \"I know but can't remember the name\" \"Refuse to answer\" ...\n $ votetsai       : num [1:1690] NA 1 0 NA NA 1 1 1 1 NA ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ green          : num [1:1690] 0 0 0 0 0 1 0 1 0 0 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ votetsai_nm    : num [1:1690] NA 1 0 NA NA 1 1 1 1 NA ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ votetsai_all   : num [1:1690] 0 1 0 0 0 1 1 1 1 NA ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ Independence   : num [1:1690] 0 1 0 1 0 0 0 1 0 0 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ Unification    : num [1:1690] 0 0 0 0 0 0 0 0 0 0 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ sq             : num [1:1690] 1 0 1 0 0 1 0 0 0 0 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ Taiwanese      : num [1:1690] 1 0 0 1 0 1 0 1 1 0 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ edu            : num [1:1690] 4 5 5 2 1 2 1 5 1 1 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ female         : num [1:1690] 1 1 0 0 1 1 0 1 1 0 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ whitecollar    : num [1:1690] 1 1 1 0 0 1 0 1 0 0 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ lowincome      : num [1:1690] 4 4 5 4 3 5 2 5 5 5 ...\n  ..- attr(*, \"label\")= chr \"How serious do you think low income of salaryman?\"\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ income         : num [1:1690] 8 7 8 5 5.5 9 1 10 2 5.5 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ income_nm      : num [1:1690] 8 7 8 5 NA 9 1 10 2 NA ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ age            : num [1:1690] 59 39 63 55 76 64 75 54 64 59 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ KMT            : num [1:1690] 0 0 1 0 0 0 0 0 0 0 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ DPP            : num [1:1690] 0 0 0 0 0 1 0 0 0 0 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ npp            : num [1:1690] 0 0 0 0 0 0 0 1 0 0 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ noparty        : num [1:1690] 1 1 0 1 1 0 1 0 1 1 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ pfp            : num [1:1690] 0 0 0 0 0 0 0 0 0 0 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ South          : num [1:1690] 0 0 0 0 0 0 0 0 0 0 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ north          : num [1:1690] 1 1 1 1 1 1 1 1 1 1 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ Minnan_father  : num [1:1690] 1 1 1 1 1 1 1 1 1 1 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ Mainland_father: num [1:1690] 0 0 0 0 0 0 0 0 0 0 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ Econ_worse     : num [1:1690] 0 0 1 1 0 1 1 1 1 1 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ Inequality     : num [1:1690] 1 1 1 1 0 1 0 1 1 1 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ inequality5    : num [1:1690] 4 5 5 5 3 5 3 5 5 5 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ econworse5     : num [1:1690] 3 3 4 5 3 4 4 5 5 5 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ Govt_for_public: num [1:1690] 1 1 1 0 0 0 0 0 0 0 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ pubwelf5       : num [1:1690] 5 5 4 1 3 2 2 1 3 2 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ Govt_dont_care : num [1:1690] 0 0 1 1 0 1 1 1 0 1 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ highincome     : num [1:1690] 1 1 1 1 NA 1 0 1 0 NA ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ votekmt        : num [1:1690] 0 0 1 0 0 0 0 0 0 0 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ votekmt_nm     : num [1:1690] NA 0 1 NA NA 0 0 0 0 NA ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ Blue           : num [1:1690] 0 0 0 0 0 0 0 0 0 0 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ Green          : num [1:1690] 0 0 0 0 0 0 0 0 0 0 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ No_Party       : num [1:1690] 0 0 0 0 0 0 0 0 0 0 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ voteblue       : num [1:1690] 0 0 1 0 0 0 0 0 0 0 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ voteblue_nm    : num [1:1690] NA 0 1 NA NA 0 0 0 0 NA ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ votedpp_1      : num [1:1690] NA 1 0 NA NA 1 1 1 1 0 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n $ votekmt_1      : num [1:1690] NA 0 1 NA NA 0 0 0 0 0 ...\n  ..- attr(*, \"format.stata\")= chr \"%9.0g\"\n\nsummary(TEDS_2016)\n\n    District         Sex             Age           Edu            Arear      \n Min.   : 201   Min.   :1.000   Min.   :1.0   Min.   :1.000   Min.   :1.000  \n 1st Qu.:1401   1st Qu.:1.000   1st Qu.:2.0   1st Qu.:2.000   1st Qu.:1.000  \n Median :6406   Median :1.000   Median :3.0   Median :3.000   Median :3.000  \n Mean   :4661   Mean   :1.486   Mean   :3.3   Mean   :3.334   Mean   :2.744  \n 3rd Qu.:6604   3rd Qu.:2.000   3rd Qu.:5.0   3rd Qu.:5.000   3rd Qu.:4.000  \n Max.   :6806   Max.   :2.000   Max.   :5.0   Max.   :9.000   Max.   :6.000  \n                                                                             \n     Career         Career8          Ethnic          Party      \n Min.   :1.000   Min.   :1.000   Min.   :1.000   Min.   : 1.00  \n 1st Qu.:1.000   1st Qu.:2.000   1st Qu.:1.000   1st Qu.: 5.00  \n Median :2.000   Median :4.000   Median :1.000   Median : 7.00  \n Mean   :2.683   Mean   :3.811   Mean   :1.658   Mean   :13.02  \n 3rd Qu.:4.000   3rd Qu.:5.000   3rd Qu.:2.000   3rd Qu.:25.00  \n Max.   :5.000   Max.   :8.000   Max.   :9.000   Max.   :26.00  \n                                                                \n    PartyID          Tondu           Tondu3           nI2       \n Min.   :1.000   Min.   :1.000   Min.   :1.000   Min.   : 1.00  \n 1st Qu.:2.000   1st Qu.:3.000   1st Qu.:2.000   1st Qu.: 1.00  \n Median :2.000   Median :4.000   Median :2.000   Median : 3.00  \n Mean   :4.522   Mean   :4.127   Mean   :2.667   Mean   :35.13  \n 3rd Qu.:9.000   3rd Qu.:5.000   3rd Qu.:3.000   3rd Qu.:98.00  \n Max.   :9.000   Max.   :9.000   Max.   :9.000   Max.   :98.00  \n                                                                \n    votetsai          green         votetsai_nm      votetsai_all   \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :1.0000   Median :0.0000   Median :1.0000   Median :1.0000  \n Mean   :0.6265   Mean   :0.3781   Mean   :0.6265   Mean   :0.5478  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n NA's   :429                       NA's   :429      NA's   :248     \n  Independence     Unification           sq           Taiwanese     \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :0.0000   Median :0.0000   Median :1.0000   Median :1.0000  \n Mean   :0.2888   Mean   :0.1225   Mean   :0.5172   Mean   :0.6272  \n 3rd Qu.:1.0000   3rd Qu.:0.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n                                                                    \n      edu            female        whitecollar       lowincome    \n Min.   :1.000   Min.   :0.0000   Min.   :0.0000   Min.   :1.000  \n 1st Qu.:2.000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:4.000  \n Median :3.000   Median :0.0000   Median :1.0000   Median :5.000  \n Mean   :3.301   Mean   :0.4864   Mean   :0.5373   Mean   :4.343  \n 3rd Qu.:5.000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:5.000  \n Max.   :5.000   Max.   :1.0000   Max.   :1.0000   Max.   :5.000  \n NA's   :10                                                       \n     income         income_nm           age              KMT        \n Min.   : 1.000   Min.   : 1.000   Min.   : 20.00   Min.   :0.0000  \n 1st Qu.: 3.000   1st Qu.: 2.000   1st Qu.: 35.00   1st Qu.:0.0000  \n Median : 5.500   Median : 5.000   Median : 49.00   Median :0.0000  \n Mean   : 5.324   Mean   : 5.281   Mean   : 49.11   Mean   :0.2296  \n 3rd Qu.: 7.000   3rd Qu.: 8.000   3rd Qu.: 61.00   3rd Qu.:0.0000  \n Max.   :10.000   Max.   :10.000   Max.   :100.00   Max.   :1.0000  \n                  NA's   :330                                       \n      DPP              npp             noparty            pfp         \n Min.   :0.0000   Min.   :0.00000   Min.   :0.0000   Min.   :0.00000  \n 1st Qu.:0.0000   1st Qu.:0.00000   1st Qu.:0.0000   1st Qu.:0.00000  \n Median :0.0000   Median :0.00000   Median :0.0000   Median :0.00000  \n Mean   :0.3497   Mean   :0.02544   Mean   :0.3716   Mean   :0.01893  \n 3rd Qu.:1.0000   3rd Qu.:0.00000   3rd Qu.:1.0000   3rd Qu.:0.00000  \n Max.   :1.0000   Max.   :1.00000   Max.   :1.0000   Max.   :1.00000  \n                                                                      \n     South            north        Minnan_father    Mainland_father \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :0.0000   Median :0.0000   Median :1.0000   Median :0.0000  \n Mean   :0.4947   Mean   :0.4799   Mean   :0.7225   Mean   :0.1024  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:0.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n                                                                    \n   Econ_worse       Inequality      inequality5      econworse5   \n Min.   :0.0000   Min.   :0.0000   Min.   :1.000   Min.   :1.000  \n 1st Qu.:0.0000   1st Qu.:1.0000   1st Qu.:4.000   1st Qu.:3.000  \n Median :1.0000   Median :1.0000   Median :5.000   Median :4.000  \n Mean   :0.5544   Mean   :0.9355   Mean   :4.495   Mean   :3.644  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:5.000   3rd Qu.:4.000  \n Max.   :1.0000   Max.   :1.0000   Max.   :5.000   Max.   :5.000  \n                                                                  \n Govt_for_public     pubwelf5     Govt_dont_care     highincome    \n Min.   :0.0000   Min.   :1.000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:2.000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :0.0000   Median :3.000   Median :0.0000   Median :1.0000  \n Mean   :0.4249   Mean   :2.877   Mean   :0.4988   Mean   :0.5765  \n 3rd Qu.:1.0000   3rd Qu.:4.000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :5.000   Max.   :1.0000   Max.   :1.0000  \n                                                   NA's   :330     \n    votekmt         votekmt_nm          Blue       Green      No_Party\n Min.   :0.0000   Min.   :0.0000   Min.   :0   Min.   :0   Min.   :0  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0   1st Qu.:0   1st Qu.:0  \n Median :0.0000   Median :0.0000   Median :0   Median :0   Median :0  \n Mean   :0.2053   Mean   :0.2752   Mean   :0   Mean   :0   Mean   :0  \n 3rd Qu.:0.0000   3rd Qu.:1.0000   3rd Qu.:0   3rd Qu.:0   3rd Qu.:0  \n Max.   :1.0000   Max.   :1.0000   Max.   :0   Max.   :0   Max.   :0  \n                  NA's   :429                                         \n    voteblue       voteblue_nm       votedpp_1        votekmt_1     \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :0.0000   Median :0.0000   Median :1.0000   Median :0.0000  \n Mean   :0.2787   Mean   :0.3735   Mean   :0.5256   Mean   :0.2309  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:0.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n                  NA's   :429      NA's   :187      NA's   :187"
  },
  {
    "objectID": "KM_Assignment02_Nakir.html#identify-problems-with-the-dataset",
    "href": "KM_Assignment02_Nakir.html#identify-problems-with-the-dataset",
    "title": "Assignment 02",
    "section": "3) Identify Problems with the Dataset",
    "text": "3) Identify Problems with the Dataset\nWe identified missing values in the dataset using colSums(is.na(...)). This tells us how many values are missing per column, which is important before running any models or summaries.\n\ncolSums(is.na(TEDS_2016))\n\n       District             Sex             Age             Edu           Arear \n              0               0               0               0               0 \n         Career         Career8          Ethnic           Party         PartyID \n              0               0               0               0               0 \n          Tondu          Tondu3             nI2        votetsai           green \n              0               0               0             429               0 \n    votetsai_nm    votetsai_all    Independence     Unification              sq \n            429             248               0               0               0 \n      Taiwanese             edu          female     whitecollar       lowincome \n              0              10               0               0               0 \n         income       income_nm             age             KMT             DPP \n              0             330               0               0               0 \n            npp         noparty             pfp           South           north \n              0               0               0               0               0 \n  Minnan_father Mainland_father      Econ_worse      Inequality     inequality5 \n              0               0               0               0               0 \n     econworse5 Govt_for_public        pubwelf5  Govt_dont_care      highincome \n              0               0               0               0             330 \n        votekmt      votekmt_nm            Blue           Green        No_Party \n              0             429               0               0               0 \n       voteblue     voteblue_nm       votedpp_1       votekmt_1 \n              0             429             187             187"
  },
  {
    "objectID": "KM_Assignment02_Nakir.html#handle-missing-values",
    "href": "KM_Assignment02_Nakir.html#handle-missing-values",
    "title": "Assignment 02",
    "section": "4) Handle Missing Values",
    "text": "4) Handle Missing Values\nWe used na.omit() to remove rows with missing values. This ensures we’re working with complete cases and helps prevent errors in analysis. After removal, we confirmed no missing values remain.\n\nTEDS_2016_cleaned &lt;- na.omit(TEDS_2016)\ncolSums(is.na(TEDS_2016_cleaned))\n\n       District             Sex             Age             Edu           Arear \n              0               0               0               0               0 \n         Career         Career8          Ethnic           Party         PartyID \n              0               0               0               0               0 \n          Tondu          Tondu3             nI2        votetsai           green \n              0               0               0               0               0 \n    votetsai_nm    votetsai_all    Independence     Unification              sq \n              0               0               0               0               0 \n      Taiwanese             edu          female     whitecollar       lowincome \n              0               0               0               0               0 \n         income       income_nm             age             KMT             DPP \n              0               0               0               0               0 \n            npp         noparty             pfp           South           north \n              0               0               0               0               0 \n  Minnan_father Mainland_father      Econ_worse      Inequality     inequality5 \n              0               0               0               0               0 \n     econworse5 Govt_for_public        pubwelf5  Govt_dont_care      highincome \n              0               0               0               0               0 \n        votekmt      votekmt_nm            Blue           Green        No_Party \n              0               0               0               0               0 \n       voteblue     voteblue_nm       votedpp_1       votekmt_1 \n              0               0               0               0"
  },
  {
    "objectID": "KM_Assignment02_Nakir.html#explore-relationship-between-tondu-and-other-variables",
    "href": "KM_Assignment02_Nakir.html#explore-relationship-between-tondu-and-other-variables",
    "title": "Assignment 02",
    "section": "5) Explore Relationship Between Tondu and Other Variables",
    "text": "5) Explore Relationship Between Tondu and Other Variables\nWe ran a linear regression to examine how Tondu is affected by predictors like gender, party support (DPP), age, income, education, ethnic identity, and economic perception.\nThe regression model helps quantify which variables have significant effects on Tondu.\n\nTEDS_2016_cleaned$female &lt;- as.factor(TEDS_2016_cleaned$female)\nTEDS_2016_cleaned$DPP &lt;- as.factor(TEDS_2016_cleaned$DPP)\nTEDS_2016_cleaned$Taiwanese &lt;- as.factor(TEDS_2016_cleaned$Taiwanese)\nTEDS_2016_cleaned$Econ_worse &lt;- as.factor(TEDS_2016_cleaned$Econ_worse)\n\nmodel &lt;- lm(Tondu ~ female + DPP + age + income + edu + Taiwanese + Econ_worse, data = TEDS_2016_cleaned)\nsummary(model)\n\n\nCall:\nlm(formula = Tondu ~ female + DPP + age + income + edu + Taiwanese + \n    Econ_worse, data = TEDS_2016_cleaned)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.4565 -1.0357 -0.0931  0.7639  5.8369 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.5696493  0.2952361  12.091  &lt; 2e-16 ***\nfemale1      0.2033016  0.0847726   2.398   0.0166 *  \nDPP1         0.3935885  0.0927837   4.242 2.41e-05 ***\nage         -0.0003988  0.0034532  -0.115   0.9081    \nincome      -0.0351503  0.0150094  -2.342   0.0194 *  \nedu         -0.0641217  0.0393935  -1.628   0.1039    \nTaiwanese1   0.9346107  0.0962575   9.709  &lt; 2e-16 ***\nEcon_worse1 -0.1535299  0.0872766  -1.759   0.0788 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.363 on 1066 degrees of freedom\nMultiple R-squared:  0.163, Adjusted R-squared:  0.1575 \nF-statistic: 29.66 on 7 and 1066 DF,  p-value: &lt; 2.2e-16\n\n\n\nVisualization of Relationships\nWe used boxplots to explore how the variable Tondu (support for unification or independence) varies by categorical variables such as gender and DPP affiliation.\n\nThe first plot shows differences in Tondu scores between male and female respondents.\nThe second plot compares Tondu scores for people who do or do not support the DPP.\n\n\nggplot(TEDS_2016_cleaned, aes(x = female, y = Tondu, fill = female)) +\n  geom_boxplot() +\n  labs(title = \"Tondu by Gender\", x = \"Female\", y = \"Tondu\") +\n  theme_minimal()\n\n\n\n\n\n\n\nggplot(TEDS_2016_cleaned, aes(x = DPP, y = Tondu, fill = DPP)) +\n  geom_boxplot() +\n  labs(title = \"Tondu by DPP Affiliation\", x = \"DPP\", y = \"Tondu\") +\n  theme_minimal()"
  },
  {
    "objectID": "KM_Assignment02_Nakir.html#explore-the-votetsai-variable",
    "href": "KM_Assignment02_Nakir.html#explore-the-votetsai-variable",
    "title": "Assignment 02",
    "section": "6) Explore the votetsai Variable",
    "text": "6) Explore the votetsai Variable\nThe variable votetsai shows whether a respondent voted for Tsai Ing-wen. A bar chart helps us understand how common support for Tsai is among the respondents.\n\nggplot(na.omit(TEDS_2016_cleaned), aes(x = factor(votetsai))) +\n  geom_bar(fill = \"steelblue\") +\n  labs(title = \"Distribution of votetsai\", x = \"Vote for Tsai Ing-wen\", y = \"Count\") +\n  theme_minimal()"
  },
  {
    "objectID": "KM_Assignment02_Nakir.html#frequency-table-and-bar-chart-for-tondu",
    "href": "KM_Assignment02_Nakir.html#frequency-table-and-bar-chart-for-tondu",
    "title": "Assignment 02",
    "section": "7) Frequency Table and Bar Chart for Tondu",
    "text": "7) Frequency Table and Bar Chart for Tondu\nTo better understand the distribution of political preferences in Taiwan, we recoded the numeric Tondu variable into labeled categories such as: - “Unification now”, “Independence now”, “Status quo forever”, etc.\nThen we used a bar chart to display the frequency of each preference. This helps visually compare how common each political stance is in the dataset.\n\nTEDS_2016$Tondu &lt;- factor(TEDS_2016$Tondu,\n                          labels = c(\"Unification now\",\n                                     \"Status quo, unif. in future\",\n                                     \"Status quo, decide later\",\n                                     \"Status quo forever\",\n                                     \"Status quo, indep. in future\",\n                                     \"Independence now\",\n                                     \"No response\"))\n\ntable(TEDS_2016$Tondu)\n\n\n             Unification now  Status quo, unif. in future \n                          27                          180 \n    Status quo, decide later           Status quo forever \n                         546                          328 \nStatus quo, indep. in future             Independence now \n                         380                          108 \n                 No response \n                         121 \n\nggplot(data.frame(Tondu = TEDS_2016$Tondu), aes(x = Tondu)) +\n  geom_bar(fill = \"coral\") +\n  theme_minimal() +\n  labs(title = \"Bar Chart of Tondu Responses\", x = \"Tondu\", y = \"Count\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))"
  }
]